common_args:
  training_type: "simulation"
  random_seed: 0

data_args:
  dataset: "reddit" # Dataset: openImg, google_speech, reddit
  data_cache_dir: "/datasets/FedScale/reddit/reddit"    # Path of the dataset
  data_map_file: "/datasets/FedScale/reddit/reddit/client_data_mapping/train.csv"              # Allocation of data to each client, turn to iid setting if not provided
  partition_method: "hetero"
  partition_alpha: 0.5
  filter_less: 21
  #task: "nlp"
  #block_size: 64
  #mlm_probability: 0.15
  #overwrite_cache: False
  #num_class: 10000

model_args:
  model: "albert-base-v2" # Models: e.g., shufflenet_v2_x2_0, mobilenet_v2, resnet34, albert-base-v2

train_args:
  federated_optimizer: "FedAvg_seq"
  client_id_list: "[]"
  client_num_in_total: 3400
  client_num_per_round: 100 # Number of participants per round, we use K=100 in our paper, large K will be much slower
  comm_round: 405
  epochs: 5
  batch_size: 20
  #train_batch_size: 20
  #test_batch_size: 20
  client_optimizer: sgd
  learning_rate: 0.0005
  weight_decay: 0.0005
  #lr_schedule: None

validation_args:
  frequency_of_the_test: 400

device_args:
  worker_num: 4
  using_gpu: true
  gpu_mapping_file: config/gpu_mapping.yaml
  gpu_mapping_key: mapping_default
  gpu_util_parse: "gpu1:0,0,2,1;gpu2:0,0,1,1" # TODO

comm_args:
  backend: "MPI"
  is_mobile: 0


tracking_args:
  log_file_dir: ./log
  enable_wandb: false
  wandb_key: a695c6339ff2ce00889a1fc05f45c3f84eccda3d
  wandb_project: fedml
  wandb_name: fedml_torch_fedavg_seq_reddit

